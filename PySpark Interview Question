PySpark Data Engineer Interview experience at Big 4 - KPMG India
Deloitte EY PwC (4 years of experience)
Introduction:
1. Can you provide an overview of your experience working with PySpark and big data processing?
2. What motivated you to specialize in PySpark, and how have you applied it in your previous roles?
PySpark Basics:
3. Explain the basic architecture of PySpark.
4. How does PySpark relate to Apache Spark, and what advantages does it offer in distributed data processing?
DataFrame Operations:
5. Describe the difference between a DataFrame and an RDD in PySpark.
6. Can you explain transformations and actions in PySpark DataFrames?
7. Provide examples of PySpark DataFrame operations you frequently use.
Optimizing PySpark Jobs:
8. How do you optimize the performance of PySpark jobs?
9. Can you discuss techniques for handling skewed data in PySpark?
Data Serialization and Compression:
10. Explain how data serialization works in PySpark.
11. Discuss the significance of choosing the right compression codec for your PySpark applications.
Handling Missing Data:
12. How do you deal with missing or null values in PySpark DataFrames?
13. Are there any specific strategies or functions you prefer for handling missing data?
Working with PySpark SQL:
14. Describe your experience with PySpark SQL.
15. How do you execute SQL queries on PySpark DataFrames?
Broadcasting in PySpark:
16. What is broadcasting, and how is it useful in PySpark?
17. Provide an example scenario where broadcasting can significantly improve performance.
PySpark Machine Learning:
18. Discuss your experience with PySpark's MLlib.
19. Can you give examples of machine learning algorithms you've implemented using PySpark?
Job Monitoring and Logging:
20. How do you monitor and troubleshoot PySpark Jobs?
21. Describe the importance of logging in PySpark applications.
Integration with Other Technologies:
22. Have you integrated PySpark with other big data technologies or databases? If so, please provide examples.
23. How do you handle data transfer between PySpark and external systems?
Real-world Project Scenario:
24. Explain the project that you worked on in your previous organizations.
25. Describe a challenging PySpark project you've worked on. What were the key challenges, and how did you overcome them?
Cluster Management:
26. Explain your experience with cluster management in PySpark.
27. How do you scale PySpark applications in a cluster environment?
PySpark Ecosystem:
28. Can you name and briefly describe some popular libraries or tools in the PySpark ecosystem, apart from the core PySpark functionality?








Scenario-based questions that are typically asked in Databricks interviews, along with general approaches to answering them.
Here's a list of potential questions and brief answer outlines:
1. Scenario: You have a large dataset with skewed data. How would you optimize a join operation?
Answer:
- Utilize Adaptive Query Execution (AQE) for dynamic optimization
- Consider broadcast joins for smaller tables
- Use salting techniques to distribute skewed keys
- Implement bucketing on join columns
2. Scenario: You need to process a stream of data in near real-time. How would you approach this using Databricks?
Answer:
- Use Structured Streaming in Databricks
- Set up a streaming source (e.g., Kafka, Event Hubs)
- Define the streaming query and output sink
- Implement watermarking and windowing for late data handling
3. Scenario: You're dealing with sensitive data. How would you ensure data security in Databricks?
Answer:
- Implement table access control (ACLs)
- Use Databricks secrets for managing credentials
- Enable encryption at rest and in transit
- Implement column-level encryption for highly sensitive data
- Use Databricks' integration with Azure AD or AWS IAM for authentication
4. Scenario: Your Spark job is running slowly. How would you diagnose and optimize it?
Answer:
- Check the Spark Ul for stage details and bottlenecks
- Look for data skew, shuffle operations, or spilling to disk
- Optimize join operations (broadcast vs. shuffle hash join)
- Adjust partitioning and caching strategies
- Use Databricks' Query Optimization Advisor
5. Scenario: You need to implement a machine learning pipeline in Databricks. How would you approach this?
Answer:
- Use MLflow for experiment tracking and model management
- Leverage Spark MLlib for distributed ML algorithms
- Implement feature engineering using Spark SQL and DataFrame operations
- Use Databricks AutoML for quick prototyping
- Set up model serving using MLflow and Databricks
Model Serving
6. Scenario: You're migrating an on-premises data warehouse to Databricks. What steps would you take?
Answer:
- Assess current data and workloads
- Design a new data architecture using Delta Lake
- Use Databricks' ETL tools for data migration
- Implement slowly changing dimensions using Delta
Lake merge operations
- Set up proper access controls and governance
- Optimize query performance using Databricks SQL warehouses
Remember, for any Databricks interview, it's crucial to demonstrate:
- Understanding of Apache Spark and its ecosystem
- Familiarity with Databricks-specific features
- Knowledge of big data processing techniques
- Awareness of performance optimization strategies
- Understanding of cloud architectures (AWS, Azure, or GCP)





Recently i got interviewed by MNC for Data Analyst.
Questions -
1) What is SQL. What are the different joins we can used ?
2) Gave me the three tables randomly make by interviewer and told me to apply the joins and give me the result ?
3) Define where and having clause.? They gave me a table and conditions then we have to apply clause?
Where clause is used with Group by or not?
4) Rate yourself in Powe BI, SQL, Microsoft Excel out of 10, and when i answered they told me rate yourself always high.
5)Define different types of Look up function?
Difference between Lookup and index function in excel, You have to show how these function works with an example?
6) What formulas you know in excel? Define Advance Excel and what's the use of it ?
7)Define Nested if and Ifs function with an example?
8) Define Power Bl and ETL?
9) Define your projects? What type of analysis you did in your Projects?
10)How do you do data cleansing? How to deal with missing values?
11) Define Pandas? What is Statistical Analysis? What types of Statistical Analysis you have done?
12) Define Mean, Median, Mode?






If you prepare these questions diligently, you can increase your chances of selection by 60%.
+ Follow
Here are the top 10 advanced SQL interview questions:
• Write a query using a Common Table Expression
(CTE) to achieve a complex data transformation.
• Explain the concept of window functions and provide an example using RANK or LAG.
Optimize a slow-running query by identifying appropriate indexes.
Write a stored procedure that takes parameters and performs specific data manipulation tasks.
• Explain the concept of database normalization and its benefits for data integrity.
• Write a query that finds duplicate rows in a table based on specific columns.
* Utilize temporary tables to perform multi-step data transformations within a single query.
• Write a query that joins multiple tables with complex relationships (e.g., self-joins).
• Explain the differences between INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN with examples.
I Write a query that aggregates data based on hierarchical relationships between records.




Planning for Data Engineering Interview.
Focus on SQL & Python first. Here are some important questions which you should know.
Important SQL questions
1- Find out nth Order/Salary from the tables.
2- Find the no of output records in each join from given Table 1 & Table 2
3- YOY,MOM Growth related questions.
4- Find out Employee ,Manager Hierarchy (Self join related question) or
Employees who are earning more than managers.
5- RANK,DENSERANK related questions
6- Some row level scanning medium to complex questions using CTE or recursive CTE, like (Missing no /Missing Item from the list etc.)
7- No of matches played by every team or Source to Destination flight combination using CROSS JOIN.
8-Use window functions to perform advanced analytical tasks, such as calculating moving averages or detecting outliers.
9- Implement logic to handle hierarchical data, such as finding all descendants of a given node in a tree structure.
10-Identify and remove duplicate records from a table.
Important Python questions
1- Reversing a String using an Extended Slicing techniques.
2- Count Vowels from Given words.
3- Find the highest occurrences of each word from string and sort them in order.
4- Remove Duplicates from List.
5-Sort a List without using Sort keyword.
6-Find the pair of numbers in this list whose sum is n no.
7-Find the max and min no in the list without using inbuilt functions.
8-Calculate the Intersection of Two Lists without using Built-in Functions
9-Write Python code to make API requests to a public
API (e.g., weather API) and process the JSON response.
10-Implement a function to fetch data from a database table, perform data manipulation, and update the database.






Important Interview Question On Spark
ニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニ
1. Difference between RDD & Dataframes
2. What are the challenges you face in spark?
3. What is difference between reduceByKey & groupByKey?
4. What is the difference between Persist and Cache?
5. What is the Advantage of a Parquet File?
6. What is a Broadcast Join ?
7. What is Difference between Coalesce and Repartition?
8. What are the roles and responsibility of driver in spark Architecture?
9. What is meant by Data Skewness? How is it deal?
10. What are the optimisation techniques used in Spark?
11. What is Difference Between Map and FlatMap?
12. What are accumulator and BroadCast Variables?
13. What is a OOM Issue, how to deal it?
14. what are tranformation in spark? Type of Transformation?
15. Tell me some action in spark that you used ?
16. What is the role of Catalyst Optimizer ?
17. what is the checkpointing?
18. Cache and persist
19. What do you understand by Lazy Evaluation?
20. How to convert Rdd to Dataframe?
21. How to Dataframe to Dataset.
22. What makes Spark better than Hadoop?
23. How can you read a CSV file without using an external schema?
24. What is the difference between Narrow Transformation and Wide Transformation?
25. What are the different parameters that can be passed while Spark-submit?
26. What are Global Temp View and Temp View?
27. How can you add two new columns to a Data frame with some calculated values?
28. Avro Vs ORC, which one do you prefer?
29. What are the different types of joins in Spark?
30. Can you explain Anti join and Semi join?
31. What is the difference between Order By, Sort By, and Cluster By?
32. Data Frame vs Dataset in spark?
33. 4. What are the join strategies in Spark
34. What happens in Cluster deployment mode and Client deployment mode
35. What are the parameters you have used in spark-submit
36. How do you add a new column in Spark
37. How do you drop a column in Spark
38. What is difference between map and flatmap?
39. What is skew partitions?
40. What is DAG and Lineage in Spark?
41. What is the difference between RDD and Dataframe?
42. Where we can find the spark application logs.
43. What is the difference between reduceByKey and groupByKey?
44. what is spark optimization?
45. What are shared variables in spark
46. What is a broadcast variable
47. Why spark instead of Hive
48. what is cache
49. Tell me the steps to read a file in spark
50. How do you handle 10 GB file in spark, how do you optimize it?




Data Engineer Scenario based interview !!
+ Follow
Scenario 1:
Interviewer: Can you design a data warehouse for an e-commerce company with 10 million customers and 1 million orders per day?
Candidate: Yes, I would design a data warehouse using Azure Synapse Analytics or Amazon Redshift, with a star schema architecture and appropriate indexing and partitioning to handle the large volume of data.
Scenario 2:
Interviewer: How would you optimize a slow-performing query that takes 10 minutes to execute?
Candidate: I would analyze the query plan, identify performance bottlenecks, and apply optimization techniques like indexing, caching, and query rewriting to reduce execution time to less than 1 minute.
Scenario 3:
Interviewer: Can you integrate data from 5 different sources, including AP/s, databases, and files, into a single data platform?
Candidate: Yes, I would use Azure Data Factory or Apache NiFi to integrate the data sources, transform and cleanse the data as needed, and load it into a unified data platform like Azure Data Lake Storage or Amazon S3.
Scenario 4:
Interviewer: How would you ensure data security and compliance with regulations like GDPR and HIPAA?
Candidate: I would implement encryption, access controls, data masking, and auditing to ensure data security and compliance, and regularly monitor and update security measures to ensure ongoing compliance.
Scenario 5:
Interviewer: Can you design a real-time data streaming platform to process 1 million events per second?
Candidate: Yes, I would design a platform using Apache Kafka or Amazon Kinesis, with appropriate clustering, partitioning, and replication to handle the high volume of data, and ensure real-time processing and analytics.
Some additional questions and figures:
- Interviewer: How do you handle data quality issues in a data warehouse?
Candidate: I would implement data validation, data cleansing, and data quality checks to ensure data accuracy and completeness, and regularly monitor and improve data quality.
- Interviewer: Can you optimize data storage costs for a large data lake?
Candidate: Yes, I would use data compression, data deduplication, and tiered storage to reduce storage costs by up to 50%.
- Interviewer: How do you ensure data governance and compliance across multiple teams and departments?
Candidate: I would establish clear data governance policies, procedures, and standards, and regularly monitor and enforce compliance across teams and departments.


Recently asked scenario-based Power BI interview question at Accenture
I have a shared folder with files for 6 months, let's say from January to June. Whenever the file for July arrives, my Power Bl report should take files from February to July. How can I achieve this?
Answer:
→ First, make sure each data file has a date column.
→ Load your 6 months of data using the folder connector in Power BI.
- After cleaning up and aligning the data in the query editor, create a custom column using the formula below:
let
CurrentDate = Date.From(DateTime.LocalNow), //
Get today's date
FirstDateOfCurrentMonth =
Date.StartOfMonth(CurrentDate),
// Get the first day of the current month
FirstDateOfSixMonthsAgo =
Date.AddMonths(FirstDateOfCurrentMonth, -6) // Get the first day of six months ago in
if [Date] >= FirstDateOfSixMonthsAgo then "Include" else "Exclude"
→ Here, replace '[Date] with your date column's name.
→ Filter the data to keep only rows where the custom column equals Include.
→ Click Close & Apply to load the filtered data into
Power BI.
→ As soon as you add the July file to the folder, the custom column will automatically exclude the January data.
→ The filter ensures only the last 6 months' data (February to July) is included.
→ When you add the August file, the custom column will exclude February and include March to August.
→ That's it! Your Power Bl report will always show the most recent 6 months of data dynamically.
If you want a non-dynamic workaround, let me know, and I'll share that in the next post.
I am sharing real interview questions asked in companies nowadays to help you prepare more practically for interviews.




